{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caquWM7G1-7p"
      },
      "source": [
        "<center><h1>Restoring ancient text using deep learning</h1>\n",
        "<h2>A case study on Greek epigraphy</h2>\n",
        "\n",
        "Yannis Assael<sup>*</sup>, Thea Sommerschield<sup>*</sup>, Jonathan Prag\n",
        "</center>\n",
        "\n",
        "---\n",
        "\n",
        "Ancient history relies on disciplines such as epigraphy, the study of ancient inscribed texts, for evidence of the recorded past. However, these texts, \"inscriptions\", are often damaged over the centuries, and illegible parts of the text must be restored by specialists, known as epigraphists. This work presents a novel assistive method for providing text restorations using deep neural networks.To the best of our knowledge, Pythia is the first ancient text restoration model that recovers missing characters from a damaged text input. Its architecture is carefully designed to handle long-term context information, and deal efficiently with missing or corrupted character and word representations. To train it, we wrote a non-trivial pipeline to convert PHI, the largest digital corpus of ancient Greek inscriptions, to machine actionable text, which we call PHI-ML. On PHI-ML, Pythia's predictions achieve a 30.1% character error rate, compared to the 57.3% of human epigraphists. Moreover, in 73.5% of cases the ground-truth sequence was among the Top-20 hypotheses of Pythia, which effectively demonstrates the impact of such an assistive method on the field of digital epigraphy, and sets the state-of-the-art in ancient text restoration.\n",
        "\n",
        "### References\n",
        "\n",
        "- [arXiv pre-print](https://arxiv.org/abs/1910.06262)\n",
        "- [EMNLP-IJCNLP 2019](https://www.aclweb.org/anthology/D19-1668)\n",
        "\n",
        "When using any of the source code of this project please cite:\n",
        "```\n",
        "@inproceedings{assael2019restoring,\n",
        "  title={Restoring ancient text using deep learning: a case study on {Greek} epigraphy},\n",
        "  author={Assael, Yannis and Sommerschield, Thea and Prag, Jonathan},\n",
        "  booktitle={Empirical Methods in Natural Language Processing},\n",
        "  pages={6369--6376},\n",
        "  year={2019}\n",
        "}\n",
        "```\n",
        "\n",
        "#### License\n",
        "\n",
        "```\n",
        "Copyright 2019 Google LLC, Thea Sommerschield, Jonathan Prag\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "```\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxOfGDWx5Wir"
      },
      "source": [
        "# Interactive notebook instructions\n",
        "\n",
        "Χαῖρε (or welcome) to the Interactive Notebook of Pythia.\n",
        "\n",
        "Please follow the inscriptions below to begin restoring ancient Greek inscriptions.\n",
        "\n",
        "1. Create a copy of this notebook to allow editing (Click on the top-left \"**Open in playground**\" mode or \"File\" -> \"Save a copy in Drive\")\n",
        "1. Execute the cells below using shift + enter to prepare the download and initiate Pythia.\n",
        "2. In the section **Imports and parameters**, the cells:\n",
        "  - load the imports;\n",
        "  - set the default character prediction parameters.\n",
        "3. In the section **Create and load Pythia**, the cells:\n",
        "  - download a pre-trained bi-word epigraphy model;\n",
        "  - define the alphabet and the text processing scripts;\n",
        "  - load the vocabulary;\n",
        "  - create an instance of the model, and\n",
        "  - the auxiliary visualisation functions.\n",
        "\n",
        "3. In the **Pythia** section, the interactive forms can be used to get the top models predictions and visualise Pythia's attention weights. You can input your own text in the \"input text\" fields, which are currently filled with example texts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzkTnANNwAgD"
      },
      "outputs": [],
      "source": [
        "#@title Imports (takes a while)\n",
        "\n",
        "import argparse\n",
        "import html\n",
        "import importlib\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import logging\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "from IPython.core.display import display, HTML\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "np.seterr(all='ignore')\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "tf.compat.v1.disable_v2_behavior()\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWYezSkuwAgJ"
      },
      "outputs": [],
      "source": [
        "#@title Parameters\n",
        "pred_char_min = 1 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "pred_char_max = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "p = argparse.ArgumentParser(prog='Pythia', description='visualise')\n",
        "# positional args:\n",
        "p.add_argument('--dataset', default=\"greek_epigraphy_dict.p\", type=str, help='dataset file')\n",
        "p.add_argument('--load_checkpoint', default=os.getcwd() + '/model_biword_epigraphy/model.ckpt-830000', type=str, help='load from checkpoint')\n",
        "p.add_argument('--batch_size', default=1, type=int, help='batch size')\n",
        "p.add_argument('--learning_rate', default=1e-3, type=float, help='learning rate')\n",
        "p.add_argument('--grad_clip', default=5., type=float, help='gradient norm clipping')\n",
        "p.add_argument('--beam_width', default=100, type=int, help='beam search width')\n",
        "p.add_argument(\"--log_samples\", default=False, type=bool, metavar='N', help='log samples')\n",
        "p.add_argument('--eval_samples', default=6400, type=int, help='number of evaluation samples')\n",
        "p.add_argument('--test_iterations', default=1, type=int, help='number of training iterations')\n",
        "p.add_argument('--context_char_min', default=-1, type=int, help='minimum context characters')\n",
        "p.add_argument('--context_char_max', default=1000, type=int, help='minimum context characters')\n",
        "p.add_argument('--pred_char_min', default=pred_char_min, type=int, help='minimum pred characters')\n",
        "p.add_argument('--pred_char_max', default=pred_char_max, type=int, help='minimum pred characters')\n",
        "p.add_argument('--missing_char_min', default=0, type=int, help='minimum missing characters')\n",
        "p.add_argument('--missing_char_max', default=0, type=int, help='minimum missing characters')\n",
        "p.add_argument('--pred_guess', default=False, type=bool, help='predict guessed characters')\n",
        "p.add_argument('--loglevel', default='INFO', type=str, metavar='LEVEL',\n",
        "               help=('Log level, will be overwritten by --debug. (DEBUG/INFO/WARN)'))\n",
        "FLAGS = p.parse_args(args=[])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVMbzu1gwAgP"
      },
      "source": [
        "# Download and load Pythia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFLntSeQwAgS"
      },
      "outputs": [],
      "source": [
        "#@title Alphabet\n",
        "class Alphabet():  # () if for passing arguments\n",
        "  # with the = in the arguments you are setting these as defaults; init = initialise\n",
        "  def __init__(self, alphabet, numerals='0123456789', punctuation='.', space=' ', pred='_', missing='-',\n",
        "               pad='#', unk='^', sos='<', eos='>', sog='[', eog=']', wordlist_path=None, wordlist_size=100000):\n",
        "    self.alphabet = list(alphabet)  # alph\n",
        "    self.numerals = list(numerals)  # num\n",
        "    self.punctuation = list(punctuation)  # punt\n",
        "    self.space = space  # spacing\n",
        "    self.pred = pred  # pred char to be predicted\n",
        "    self.missing = missing  # missing char\n",
        "    self.pad = pad  # padding (spaces to right of string)\n",
        "    self.unk = unk  # unknown char\n",
        "    self.sos = sos  # start of sentence\n",
        "    self.eos = eos  # end of entence\n",
        "    self.sog = sog  # start of guess\n",
        "    self.eog = eog  # end of guess\n",
        "\n",
        "    # Define wordlist mapping\n",
        "    if wordlist_path is None:\n",
        "      self.idx2word = None\n",
        "      self.word2idx = None\n",
        "    else:\n",
        "      with open(wordlist_path, \"r\") as f:\n",
        "        self.idx2word = [self.sos,\n",
        "                         self.eos,\n",
        "                         self.pad,\n",
        "                         self.unk] + [w_c.split('\\t')[0] for w_c in f.read().split('\\n')[:wordlist_size]]\n",
        "      self.word2idx = {self.idx2word[i]: i for i in range(len(self.idx2word))}\n",
        "\n",
        "    # Define vocab mapping\n",
        "    self.idx2char = [self.sos,\n",
        "                     self.eos,\n",
        "                     self.pad,\n",
        "                     self.unk] + self.alphabet + self.numerals + self.punctuation + [\n",
        "                      self.space,\n",
        "                      self.missing,\n",
        "                      self.pred]\n",
        "    self.char2idx = {self.idx2char[i]: i for i in range(len(self.idx2char))}\n",
        "\n",
        "    # Define special character indices\n",
        "    self.pad_idx = self.char2idx[pad]\n",
        "    self.unk_idx = self.char2idx[unk]\n",
        "    self.sos_idx = self.char2idx[sos]\n",
        "    self.eos_idx = self.char2idx[eos]\n",
        "\n",
        "  def filter(self, t):\n",
        "    return t\n",
        "\n",
        "\n",
        "class GreekAlphabet(Alphabet):\n",
        "  def __init__(self):\n",
        "    super().__init__(alphabet=\n",
        "                     'ΐάέήίΰαβγδεζηθικλμνξοπρςστυφχψωϊϋόύώϙϛἀἁἂἃἄἅἆἇἐἑἒἓἔἕἠἡἢἣἤἥἦἧἰἱἲἳἴἵἶἷὀὁὂὃὄὅὐὑὒὓὔὕὖὗὠὡὢὣὤὥὦὧὰὲὴὶὸὺὼᾀᾁᾂᾃᾄᾅᾆᾇᾐᾑᾒᾓᾔᾕᾖᾗᾠᾡᾢᾣᾤᾥᾦᾧᾰᾱᾲᾳᾴᾶᾷῂῃῄῆῇῐῑῖῠῡῤῥῦῲῳῴῶῷ',\n",
        "                     wordlist_path=os.getcwd() + \"/datasets/greek_text_and_epigraphy_wordlist.txt\",\n",
        "                     wordlist_size=100000)  # ͱͳͷϝϟϡ϶ϸϻ\n",
        "    self.tonos_to_oxia = {\n",
        "      # tonos   : #oxia\n",
        "      \"\\u0386\": \"\\u1FBB\",  # capital letter alpha\n",
        "      \"\\u0388\": \"\\u1FC9\",  # capital letter epsilon\n",
        "      \"\\u0389\": \"\\u1FCB\",  # capital letter eta\n",
        "      \"\\u038C\": \"\\u1FF9\",  # capital letter omicron\n",
        "      \"\\u038A\": \"\\u1FDB\",  # capital letter iota\n",
        "      \"\\u038E\": \"\\u1FF9\",  # capital letter upsilon\n",
        "      \"\\u038F\": \"\\u1FFB\",  # capital letter omega\n",
        "      \"\\u03AC\": \"\\u1F71\",  # small letter alpha\n",
        "      \"\\u03AD\": \"\\u1F73\",  # small letter epsilon\n",
        "      \"\\u03AE\": \"\\u1F75\",  # small letter eta\n",
        "      \"\\u0390\": \"\\u1FD3\",  # small letter iota with dialytika and tonos/oxia\n",
        "      \"\\u03AF\": \"\\u1F77\",  # small letter iota\n",
        "      \"\\u03CC\": \"\\u1F79\",  # small letter omicron\n",
        "      \"\\u03B0\": \"\\u1FE3\",  # small letter upsilon with with dialytika and tonos/oxia\n",
        "      \"\\u03CD\": \"\\u1F7B\",  # small letter upsilon\n",
        "      \"\\u03CE\": \"\\u1F7D\"  # small letter omega\n",
        "    }\n",
        "    self.oxia_to_tonos = {v: k for k, v in self.tonos_to_oxia.items()}\n",
        "\n",
        "  def filter(self, t):  # override previous filter function\n",
        "    # lowercase\n",
        "    t = t.lower()\n",
        "\n",
        "    # replace dot below\n",
        "    t = t.replace(u'\\u0323', '')\n",
        "\n",
        "    # replace perispomeni\n",
        "    t = t.replace(u'\\u0342', '')\n",
        "\n",
        "    # replace ending sigma\n",
        "    t = re.sub(r'([\\w\\[\\]])σ(?![\\[\\]])(\\b)', r'\\1ς\\2', t)\n",
        "\n",
        "    # replace oxia with tonos\n",
        "    for oxia, tonos in self.oxia_to_tonos.items():\n",
        "      t = t.replace(oxia, tonos)\n",
        "\n",
        "    # replace h\n",
        "    h_patterns = {\n",
        "      # input: #target\n",
        "      \"ε\": \"ἑ\",\n",
        "      \"ὲ\": \"ἓ\",\n",
        "      \"έ\": \"ἕ\",\n",
        "\n",
        "      \"α\": \"ἁ\",\n",
        "      \"ὰ\": \"ἃ\",\n",
        "      \"ά\": \"ἅ\",\n",
        "      \"ᾶ\": \"ἇ\",\n",
        "\n",
        "      \"ι\": \"ἱ\",\n",
        "      \"ὶ\": \"ἳ\",\n",
        "      \"ί\": \"ἵ\",\n",
        "      \"ῖ\": \"ἷ\",\n",
        "\n",
        "      \"ο\": \"ὁ\",\n",
        "      \"ό\": \"ὅ\",\n",
        "      \"ὸ\": \"ὃ\",\n",
        "\n",
        "      \"υ\": \"ὑ\",\n",
        "      \"ὺ\": \"ὓ\",\n",
        "      \"ύ\": \"ὕ\",\n",
        "      \"ῦ\": \"ὗ\",\n",
        "\n",
        "      \"ὴ\": \"ἣ\",\n",
        "      \"η\": \"ἡ\",\n",
        "      \"ή\": \"ἥ\",\n",
        "      \"ῆ\": \"ἧ\",\n",
        "\n",
        "      \"ὼ\": \"ὣ\",\n",
        "      \"ώ\": \"ὥ\",\n",
        "      \"ω\": \"ὡ\",\n",
        "      \"ῶ\": \"ὧ\"\n",
        "    }\n",
        "\n",
        "    # iterate by keys\n",
        "    for h_in, h_tar in h_patterns.items():\n",
        "      # look up and replace h[ and h]\n",
        "      t = re.sub(r'ℎ(\\[?){}'.format(h_in), r'\\1{}'.format(h_tar), t)\n",
        "      t = re.sub(r'ℎ(\\]?){}'.format(h_in), r'{}\\1'.format(h_tar), t)\n",
        "\n",
        "    # any h left is an ἡ\n",
        "    t = re.sub(r'(\\[?)ℎ(\\]?)', r'\\1ἡ\\2'.format(h_tar), t)\n",
        "\n",
        "    return t\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-ly4tRswAgY"
      },
      "outputs": [],
      "source": [
        "#@title Vocabulary\n",
        "def text_to_idx(t, alphabet):\n",
        "  return np.array([alphabet.char2idx[c] for c in t], dtype=np.int32)\n",
        "\n",
        "\n",
        "def text_to_word_idx(t, alphabet):\n",
        "  out = np.full(len(t), alphabet.word2idx[alphabet.unk], dtype=np.int32)\n",
        "\n",
        "  for m in re.finditer(r'\\w+', t):\n",
        "    if m.group() in alphabet.word2idx:\n",
        "      out[m.start():m.end()] = alphabet.word2idx[m.group()]\n",
        "\n",
        "  return out\n",
        "\n",
        "\n",
        "def idx_to_text(idxs, alphabet):\n",
        "  idxs = np.array(idxs)\n",
        "  out = ''\n",
        "  for i in range(idxs.size):\n",
        "    idx = idxs[i]\n",
        "    if idx == alphabet.eos_idx:\n",
        "      break\n",
        "    elif idx not in [alphabet.sos_idx]:\n",
        "      out += alphabet.idx2char[idx]\n",
        "\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySshtVLgwAgi"
      },
      "outputs": [],
      "source": [
        "#@title Dataset\n",
        "class Text():\n",
        "\n",
        "  def __init__(self, path=None, sentences=[]):\n",
        "    self.path = path\n",
        "    self.sentences = sentences[:]\n",
        "\n",
        "  def __str__(self):\n",
        "    return 'path: {}, sentences: {}'.format(self.path, len(self.sentences))\n",
        "\n",
        "\n",
        "def generate_sample(config, alphabet, texts, idx=None, eos=False, pad=False, is_training=False):\n",
        "  iter = 0\n",
        "  x_idx, x_len, x_word_idx, x_word_len, y_idx, y_len = None, None, None, None, None, None\n",
        "\n",
        "  # select random text until length is satisfied\n",
        "  while True:\n",
        "    iter += 1\n",
        "\n",
        "    if idx is None:\n",
        "      t = np.random.choice(texts)\n",
        "    else:\n",
        "      t = texts[idx]\n",
        "      if iter > 1:\n",
        "        break\n",
        "    text = ' '.join([s + '.' for s in t.sentences])\n",
        "\n",
        "    # remove guess signs\n",
        "    if not config.pred_guess:\n",
        "      text = text.replace(alphabet.sog, '').replace(alphabet.eog, '')\n",
        "\n",
        "    # randomly select context size and number of characters to remove\n",
        "    context_char_max = min(max(config.context_char_min, len(text)), config.context_char_max)\n",
        "    if config.context_char_min > 0:\n",
        "      context_char_num = np.random.randint(config.context_char_min, context_char_max)\n",
        "    else:\n",
        "      context_char_num = context_char_max\n",
        "\n",
        "    if len(text.replace(alphabet.sog, '').replace(alphabet.eog, '')) < context_char_num:\n",
        "      continue\n",
        "\n",
        "    # compute start sentence\n",
        "    text_idx_start = np.random.randint(0, len(text) - context_char_num) if len(text) > context_char_num else 0\n",
        "    text_idx_end = text_idx_start + context_char_num\n",
        "\n",
        "    # keep only current text\n",
        "    text = text[text_idx_start:text_idx_end]\n",
        "\n",
        "    if config.pred_guess and not is_training:\n",
        "      # delete guess characters\n",
        "      matches = []\n",
        "      for m in re.finditer(r'%s([^%s%s]+)%s' % (\n",
        "          re.escape(alphabet.sog),\n",
        "          re.escape(alphabet.missing),\n",
        "          re.escape(alphabet.eog), re.escape(alphabet.eog)), text):\n",
        "        start = m.start() + 1\n",
        "        end = m.end() - 2\n",
        "        if config.pred_char_min <= end - start <= config.pred_char_max:\n",
        "          matches.append((m.group(1), start, end))\n",
        "\n",
        "      # skip if no matches found\n",
        "      if len(matches) == 0:\n",
        "        continue\n",
        "\n",
        "      # pick a random match\n",
        "      matches_idx = np.random.randint(len(matches))\n",
        "      (y, pred_start, pred_end) = matches[matches_idx]\n",
        "      x = list(text)\n",
        "      for i in range(pred_start, pred_end + 1):\n",
        "        x[i] = alphabet.pred\n",
        "\n",
        "      # remove guess signs\n",
        "      x = [c for c in x if c not in [alphabet.sog, alphabet.eog]]\n",
        "\n",
        "    else:\n",
        "      # delete pred characters\n",
        "      if alphabet.pred not in text:\n",
        "        pred_char_num = np.random.randint(config.pred_char_min, min(len(text), config.pred_char_max))\n",
        "        if len(text) < pred_char_num:\n",
        "          continue\n",
        "        pred_char_idx = np.random.randint(0, len(text) - pred_char_num) if len(text) > pred_char_num else 0\n",
        "        y = text[pred_char_idx:pred_char_idx + pred_char_num]\n",
        "      else:\n",
        "        y = ''\n",
        "\n",
        "      # skip if it's a missing character\n",
        "      if alphabet.missing in y:\n",
        "        continue\n",
        "\n",
        "      x = list(text)\n",
        "      if alphabet.pred not in x:\n",
        "        for i in range(pred_char_idx, pred_char_idx + pred_char_num):\n",
        "          x[i] = alphabet.pred\n",
        "\n",
        "      # hide random characters\n",
        "      if config.missing_char_max > 0 and is_training:\n",
        "        missing_char_num = np.random.randint(config.missing_char_min, min(len(text), config.missing_char_max))\n",
        "        for i in np.random.randint(0, context_char_num, missing_char_num):\n",
        "          if x[i] != alphabet.pred:\n",
        "            x[i] = alphabet.missing\n",
        "\n",
        "    # convert to string\n",
        "    x = ''.join(x)\n",
        "\n",
        "    # convert to indices\n",
        "    x_idx = text_to_idx(x, alphabet)\n",
        "    x_word_idx = text_to_word_idx(x, alphabet)\n",
        "    y_idx = text_to_idx(y, alphabet)\n",
        "    assert (len(x_idx) == len(x_word_idx))\n",
        "\n",
        "    # append eos character\n",
        "    if eos:\n",
        "      y_idx = np.concatenate((y_idx, [alphabet.eos_idx]))\n",
        "\n",
        "    # compute lengths\n",
        "    x_len = np.int32(x_idx.shape[0])\n",
        "    x_word_len = np.int32(x_word_idx.shape[0])\n",
        "    y_len = np.int32(y_idx.shape[0])\n",
        "\n",
        "    # pad sequences\n",
        "    if pad:\n",
        "      x_idx = np.pad(x_idx, (0, config.context_char_max - x_idx.size), 'constant',\n",
        "                     constant_values=(0, alphabet.eos_idx))\n",
        "      x_word_idx = np.pad(x_word_idx, (0, config.context_char_max - x_word_idx.size), 'constant',\n",
        "                          constant_values=(0, alphabet.eos_idx))\n",
        "      y_idx = np.pad(y_idx, (0, config.pred_char_max - y_idx.size + 1), 'constant',\n",
        "                     constant_values=(0, alphabet.eos_idx))\n",
        "\n",
        "    break\n",
        "\n",
        "  return {'x': x_idx, 'x_len': x_len,\n",
        "          'x_word': x_word_idx, 'x_word_len': x_word_len,\n",
        "          'y': y_idx, 'y_len': y_len}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKEZD-eiwAgz"
      },
      "outputs": [],
      "source": [
        "#@title Attention visualisation\n",
        "def softmax(x, axis=-1):\n",
        "    e_x = np.exp(x - np.max(x)) # same code\n",
        "    return e_x / e_x.sum(axis=axis, keepdims=True)\n",
        "\n",
        "\n",
        "def scale(x):\n",
        "  return (x - np.min(x)) / np.ptp(x)\n",
        "\n",
        "\n",
        "def generate_alignment(x_, y_, y_pred_0, out_tensors):\n",
        "  html_src = '''\n",
        "  <html>\n",
        "  <head>\n",
        "  <link href=\"https://fonts.googleapis.com/css?family=Roboto+Mono\" rel=\"stylesheet\">\n",
        "  <style>\n",
        "  body { font-family: 'Roboto Mono', monospace; }\n",
        "  table {\n",
        "    width:100%%;\n",
        "    table-layout: fixed;\n",
        "    font-size: 11px;\n",
        "    padding: 0;\n",
        "  }\n",
        "  table td{\n",
        "    word-wrap: break-word;\n",
        "    white-space: normal;\n",
        "  }\n",
        "  span {\n",
        "    padding: 0;\n",
        "  }\n",
        "  </style>\n",
        "  </head>\n",
        "  <body>\n",
        "  <p><b>y:</b> %s</p>\n",
        "  <p><b>y_pred:</b> %s</p>''' % (y_, y_pred_0)\n",
        "\n",
        "  for attn_layer in range(len(out_tensors.alignment_history)):\n",
        "    text = np.array(list(x_))\n",
        "    text_missing_idx = text == '_'\n",
        "    html_src += '<h2>layer {}</h2>'.format(attn_layer)\n",
        "\n",
        "    html_src += '<table>'\n",
        "    for y_idx in range(len(y_pred_0)):\n",
        "      html_src += '<tr><td width=\"20\"><b>{}</b></td><td>'.format(y_pred_0[y_idx])\n",
        "      attn = out_tensors.alignment_history[attn_layer].copy()[y_idx, 0, :len(x_)]\n",
        "\n",
        "      # rescale\n",
        "      attn[np.bitwise_not(text_missing_idx)] /= attn[np.bitwise_not(text_missing_idx)].max()\n",
        "      if (text_missing_idx).any():\n",
        "        attn[text_missing_idx] /= attn[text_missing_idx].max()\n",
        "\n",
        "      for i in range(len(text)):\n",
        "        if text[i] == '_':\n",
        "          html_src += '<span style=\"background-color: rgba(139,195,74,{:.2f});\">{}</span>'.format(attn[i], '?')\n",
        "        else:\n",
        "          html_src += '<span style=\"background-color: rgba(33,150,243,{:.2f});\">{}</span>'.format(attn[i], html.escape(text[i]))\n",
        "      html_src += '</td></tr>'\n",
        "    html_src += '</table>'\n",
        "\n",
        "  html_src += '</body></html>'\n",
        "\n",
        "  return html_src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HKwOUx_y9mw"
      },
      "outputs": [],
      "source": [
        "#@title Model\n",
        "import collections\n",
        "\n",
        "import sonnet as snt\n",
        "# import tensorflow as tf\n",
        "# from tensor2tensor.models import transformer\n",
        "# from tensor2tensor.utils.learning_rate import learning_rate_schedule\n",
        "\n",
        "\n",
        "class Model(snt.AbstractModule):\n",
        "\n",
        "  def __init__(self,\n",
        "               config,\n",
        "               alphabet,\n",
        "               name=\"model_0010w\"):\n",
        "    super(Model, self).__init__(name=name)\n",
        "    self._config = config\n",
        "    self._vocab_size_char = len(alphabet.idx2char)\n",
        "    self._vocab_size_word = len(alphabet.idx2word)\n",
        "    self._pad_idx = alphabet.pad_idx\n",
        "    self._unk_idx = alphabet.unk_idx\n",
        "    self._sos_idx = alphabet.sos_idx\n",
        "    self._eos_idx = alphabet.eos_idx\n",
        "\n",
        "  # def get_learning_rate(self):\n",
        "  #   hparams = transformer.transformer_base()\n",
        "  #   return learning_rate_schedule(hparams)\n",
        "\n",
        "  def _build(self,\n",
        "             batch,\n",
        "             keep_prob=0.8,\n",
        "             sampling_probability=0.5,\n",
        "             beam_width=0,\n",
        "             is_training=False):\n",
        "\n",
        "    # encoder\n",
        "    with tf.variable_scope('encoder'):\n",
        "      encoder_output, encoder_state = self._encoder(\n",
        "        inputs=(tf.transpose(batch['x'], [1, 0]), tf.transpose(batch['x_word'], [1, 0])),\n",
        "        lengths=batch['x_len'],\n",
        "        target_size=(self._vocab_size_char, self._vocab_size_word),\n",
        "        rnn_size_enc=512,\n",
        "        rnn_size_dec=512,\n",
        "        rnn_layers_enc=2,\n",
        "        keep_prob=keep_prob,\n",
        "        is_training=is_training)\n",
        "\n",
        "    # decoder\n",
        "    with tf.variable_scope('decoder'):\n",
        "      decoder_output = self._seq2seq(encoder_output=encoder_output,\n",
        "                                     encoder_state=encoder_state,\n",
        "                                     encoder_lengths=batch['x_len'],\n",
        "                                     target=batch['y'],\n",
        "                                     target_lengths=batch['y_len'],\n",
        "                                     target_size=self._vocab_size_char,\n",
        "                                     sos_idx=self._sos_idx,\n",
        "                                     eos_idx=self._eos_idx,\n",
        "                                     rnn_size_dec=512,\n",
        "                                     rnn_layers_dec=2,\n",
        "                                     attention_fn=tf.contrib.seq2seq.LuongAttention,\n",
        "                                     beam_width=beam_width,\n",
        "                                     sampling_probability=sampling_probability,\n",
        "                                     keep_prob=keep_prob,\n",
        "                                     is_training=is_training)\n",
        "    return decoder_output\n",
        "\n",
        "  def _encoder(self,\n",
        "               inputs,\n",
        "               lengths,\n",
        "               target_size,\n",
        "               rnn_cell=tf.contrib.rnn.LSTMBlockCell,\n",
        "               rnn_size_enc=128,\n",
        "               rnn_size_dec=128,\n",
        "               rnn_layers_enc=8,\n",
        "               keep_prob=1.,\n",
        "               is_training=False):\n",
        "\n",
        "    with tf.variable_scope('encoder'):\n",
        "      # character embedding\n",
        "      self.embedding_encoder_char = snt.Embed(target_size[0], embed_dim=rnn_size_enc, name='embedding_encoder_char')\n",
        "      inputs_char_emb = self.embedding_encoder_char(inputs[0])\n",
        "\n",
        "      # word embedding\n",
        "      self.embedding_encoder_word = snt.Embed(target_size[1], embed_dim=rnn_size_enc, name='embedding_encoder_word')\n",
        "      inputs_word_emb = self.embedding_encoder_word(inputs[1])\n",
        "\n",
        "      # combine embeddings\n",
        "      inputs_emb = tf.concat([inputs_char_emb, inputs_word_emb], axis=-1)\n",
        "\n",
        "      # rnn cells\n",
        "      cells_fw = [tf.compat.v1.nn.rnn_cell.DropoutWrapper(\n",
        "        rnn_cell(rnn_size_enc, reuse=tf.AUTO_REUSE,\n",
        "                 name='rnn_fw_%d' % l),\n",
        "        input_keep_prob=keep_prob if is_training else 1.,\n",
        "        dtype=tf.float32) for l in range(rnn_layers_enc)]\n",
        "\n",
        "      cells_bw = [tf.compat.v1.nn.rnn_cell.DropoutWrapper(\n",
        "        rnn_cell(rnn_size_enc, reuse=tf.AUTO_REUSE,\n",
        "                 name='rnn_bw_%d' % l),\n",
        "        input_keep_prob=keep_prob if is_training else 1.,\n",
        "        dtype=tf.float32) for l in range(rnn_layers_enc)]\n",
        "\n",
        "      # run bidirectional rnn\n",
        "      encoder_output, states_fw, states_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
        "        cells_fw=cells_fw,\n",
        "        cells_bw=cells_bw,\n",
        "        inputs=inputs_emb,\n",
        "        sequence_length=lengths,\n",
        "        time_major=True,\n",
        "        dtype=tf.float32)\n",
        "\n",
        "      # concatenate state of last layer\n",
        "      encoder_states = []\n",
        "      for l in range(rnn_layers_enc):\n",
        "        state_c = tf.concat(\n",
        "          values=(states_fw[l].c, states_bw[l].c),\n",
        "          axis=1)\n",
        "        state_c_bridge = tf.layers.dense(state_c, rnn_size_dec,\n",
        "                                         trainable=is_training,\n",
        "                                         name='state_c_bridge_%d' % l,\n",
        "                                         reuse=tf.AUTO_REUSE)\n",
        "        state_h = tf.concat(\n",
        "          values=(states_fw[l].h, states_bw[l].h),\n",
        "          axis=1)\n",
        "        state_h_bridge = tf.layers.dense(state_h, rnn_size_dec,\n",
        "                                         trainable=is_training,\n",
        "                                         name='state_h_bridge_%d' % l,\n",
        "                                         reuse=tf.AUTO_REUSE)\n",
        "        encoder_states.append(tf.contrib.rnn.LSTMStateTuple(c=state_c_bridge, h=state_h_bridge))\n",
        "\n",
        "    return tf.transpose(encoder_output, [1, 0, 2]), tuple(encoder_states)\n",
        "\n",
        "  def _seq2seq(self,\n",
        "               encoder_output,\n",
        "               encoder_state,\n",
        "               encoder_lengths,\n",
        "               target,\n",
        "               target_lengths,\n",
        "               target_size,\n",
        "               sos_idx,\n",
        "               eos_idx,\n",
        "               rnn_cell=tf.contrib.rnn.LSTMBlockCell,\n",
        "               rnn_size_dec=128,\n",
        "               rnn_layers_dec=3,\n",
        "               attention_fn=tf.contrib.seq2seq.LuongAttention,\n",
        "               beam_width=0,\n",
        "               sampling_probability=0.,\n",
        "               keep_prob=1.,\n",
        "               is_training=False):\n",
        "\n",
        "    with tf.variable_scope('decoder'):\n",
        "      batch_size = tf.shape(encoder_output)[0]\n",
        "\n",
        "      # decoder embedding\n",
        "      self.embedding_decoder = snt.Embed(target_size, embed_dim=rnn_size_dec, name='embedding_decoder')\n",
        "      self.sos_idx = sos_idx\n",
        "      self.eos_idx = eos_idx\n",
        "      self.sos_tokens = tf.fill([batch_size], self.sos_idx)\n",
        "\n",
        "      # beam search\n",
        "      if not is_training and beam_width > 0:\n",
        "        encoder_output = tf.contrib.seq2seq.tile_batch(encoder_output,\n",
        "                                                       multiplier=beam_width)\n",
        "        encoder_state = tf.contrib.seq2seq.tile_batch(encoder_state,\n",
        "                                                      multiplier=beam_width)\n",
        "        encoder_lengths = tf.contrib.seq2seq.tile_batch(encoder_lengths,\n",
        "                                                        multiplier=beam_width)\n",
        "\n",
        "      # define attetnion\n",
        "      self.attention_mechanism = attention_fn(rnn_size_dec,\n",
        "                                              memory=encoder_output,\n",
        "                                              memory_sequence_length=encoder_lengths,\n",
        "                                              scale=True)\n",
        "\n",
        "      # attention cell\n",
        "      attention_cell = tf.contrib.rnn.MultiRNNCell([\n",
        "        tf.compat.v1.nn.rnn_cell.DropoutWrapper(rnn_cell(rnn_size_dec, reuse=tf.AUTO_REUSE, name='rnn_%d' % l),\n",
        "                                      input_keep_prob=keep_prob if is_training else 1.,\n",
        "                                      dtype=tf.float32) for l in range(rnn_layers_dec)])\n",
        "\n",
        "      # attention wrapper\n",
        "      self.decoder_cell = tf.contrib.seq2seq.AttentionWrapper(attention_cell,\n",
        "                                                              [self.attention_mechanism] * rnn_layers_dec,\n",
        "                                                              attention_layer_size=[rnn_size_dec] * rnn_layers_dec,\n",
        "                                                              alignment_history=(not is_training and beam_width == 0))\n",
        "\n",
        "      # initial attention state\n",
        "      if not is_training and beam_width > 0:\n",
        "        bs = batch_size * beam_width\n",
        "      else:\n",
        "        bs = batch_size\n",
        "      decoder_initial_state = self.decoder_cell.zero_state(bs, tf.float32).clone(\n",
        "        cell_state=encoder_state)\n",
        "\n",
        "      # projection layer\n",
        "      self.projection_layer = tf.layers.Dense(target_size,\n",
        "                                              use_bias=False,\n",
        "                                              name='output_projection',\n",
        "                                              trainable=is_training,\n",
        "                                              _reuse=tf.AUTO_REUSE)\n",
        "\n",
        "      # training and inference helpers\n",
        "      if is_training:\n",
        "        # left pad to add sos idx\n",
        "        target_sos = tf.pad(target, [[0, 0], [1, 0]], constant_values=self.sos_idx)\n",
        "\n",
        "        # helper\n",
        "        target_emb_input = self.embedding_decoder(target_sos)\n",
        "        if sampling_probability > 0:\n",
        "          helper = tf.contrib.seq2seq.ScheduledEmbeddingTrainingHelper(\n",
        "            target_emb_input,\n",
        "            sequence_length=target_lengths,\n",
        "            embedding=self.embedding_decoder,\n",
        "            sampling_probability=sampling_probability,\n",
        "            time_major=False)\n",
        "        else:\n",
        "          helper = tf.contrib.seq2seq.TrainingHelper(target_emb_input,\n",
        "                                                     sequence_length=target_lengths,\n",
        "                                                     time_major=False)\n",
        "        # decoder\n",
        "        decoder = tf.contrib.seq2seq.BasicDecoder(cell=self.decoder_cell,\n",
        "                                                  helper=helper,\n",
        "                                                  initial_state=decoder_initial_state,\n",
        "                                                  output_layer=self.projection_layer)\n",
        "        maximum_iterations = None\n",
        "      else:\n",
        "        # inference\n",
        "        if beam_width > 0:\n",
        "          decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
        "            cell=self.decoder_cell,\n",
        "            embedding=self.embedding_decoder,\n",
        "            start_tokens=self.sos_tokens,\n",
        "            end_token=self.eos_idx,\n",
        "            initial_state=decoder_initial_state,\n",
        "            beam_width=beam_width,\n",
        "            output_layer=self.projection_layer)\n",
        "        else:\n",
        "          helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
        "            embedding=self.embedding_decoder,\n",
        "            start_tokens=self.sos_tokens,\n",
        "            end_token=self.eos_idx)\n",
        "\n",
        "          decoder = tf.contrib.seq2seq.BasicDecoder(cell=self.decoder_cell,\n",
        "                                                    helper=helper,\n",
        "                                                    initial_state=decoder_initial_state,\n",
        "                                                    output_layer=self.projection_layer)\n",
        "\n",
        "        maximum_iterations = tf.round(self._config.pred_char_max)\n",
        "\n",
        "      (final_outputs, final_state, final_sequence_lengths) = tf.contrib.seq2seq.dynamic_decode(\n",
        "        decoder,\n",
        "        output_time_major=False,\n",
        "        maximum_iterations=maximum_iterations,\n",
        "        swap_memory=True)\n",
        "\n",
        "      if is_training:\n",
        "        logits = final_outputs.rnn_output\n",
        "        sample = final_outputs.sample_id\n",
        "        alignment_history = tf.no_op()\n",
        "      else:\n",
        "        logits = tf.no_op()\n",
        "        if beam_width > 0:\n",
        "          sample = final_outputs.predicted_ids\n",
        "          alignment_history = tf.no_op()\n",
        "        else:\n",
        "          sample = final_outputs.sample_id\n",
        "\n",
        "          alignment_history = []\n",
        "          for history_array in final_state.alignment_history:\n",
        "            alignment_history.append(history_array.stack())\n",
        "          alignment_history = tuple(alignment_history)\n",
        "\n",
        "      return collections.namedtuple('Outputs', 'logits sample alignment_history')(logits=logits, sample=sample,\n",
        "                                                                                  alignment_history=alignment_history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KCGGVqNwAhH",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#@title Create graph\n",
        "alphabet = GreekAlphabet()\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# create input placeholder\n",
        "batch_tf = {\n",
        "  'x': tf.placeholder(tf.int32, shape=[1, FLAGS.context_char_max]),\n",
        "  'x_len': tf.placeholder(tf.int32, shape=[1]),\n",
        "  'x_word': tf.placeholder(tf.int32, shape=[1, FLAGS.context_char_max]),\n",
        "  'x_word_len': tf.placeholder(tf.int32, shape=[1]),\n",
        "  'y': tf.placeholder(tf.int32, shape=[1, FLAGS.pred_char_max]),\n",
        "  'y_len': tf.placeholder(tf.int32, shape=[1])\n",
        "}\n",
        "\n",
        "# create the model\n",
        "model = Model(config=FLAGS, alphabet=alphabet)\n",
        "\n",
        "# evaluation model with beam search\n",
        "graph_tensors = model(batch_tf,\n",
        "                      keep_prob=1.,\n",
        "                      sampling_probability=0,\n",
        "                      beam_width=FLAGS.beam_width,\n",
        "                      is_training=False)\n",
        "\n",
        "# evaluation model greedy\n",
        "graph_tensors_greedy = model(batch_tf,\n",
        "                             keep_prob=1.,\n",
        "                             sampling_probability=0,\n",
        "                             beam_width=0,\n",
        "                             is_training=False)\n",
        "\n",
        "# configure a checkpoint saver.\n",
        "saver = tf.compat.v1.train.Saver()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKtbDthbwAhO"
      },
      "outputs": [],
      "source": [
        "#@title Create session and restore parameters\n",
        "# create a session\n",
        "sess = tf.Session()\n",
        "\n",
        "# restore model weights from previously saved model\n",
        "saver.restore(sess, FLAGS.load_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDQiIEU7wAhU"
      },
      "source": [
        "# Pythia\n",
        "\n",
        "- Use the \"**input text**\" field to enter text in ancient greek.\n",
        "- The missing characters are denoted by the \"-\" symbol and the characters to be predicted by the \"_\" symbol.\n",
        "- For optimal usage and results avoid using additional special character notation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92-qKd74wAha"
      },
      "outputs": [],
      "source": [
        "#@title Get top predictions\n",
        "\n",
        "# μὴ κρίνετε ἵνα μὴ κριθῆτε ἐν ᾧ γὰρ κρίματι κρίνετε κριθήσεσθε καὶ ἐν ᾧ μέτρῳ μετρεῖτε μετρηθήσεται ὑμῖν τί δὲ βλέπεις τὸ κάρφος τὸ ἐν τῷ ὀφθαλμῷ τοῦ ἀδελφοῦ σου τὴν δὲ ἐν τῷ σῷ ὀφθαλμῷ δοκὸν οὐ κατανοεῖς ἢ πῶς ἐρεῖς τῷ ἀδελφῷ σου ἄφες ἐκβάλω τὸ κάρφος ἀπὸ τοῦ ὀφθαλμοῦ σου καὶ ἰδοὺ ἡ δοκὸς ἐν τῷ ὀφθαλμῷ σου ὑποκριτά ἔκβαλε πρῶτον τὴν δοκὸν ἐκ τοῦ ὀφθαλμοῦ σου καὶ τότε διαβλέψεις ἐκβαλεῖν τὸ κάρφος ἐκ τοῦ ὀφθαλμοῦ τοῦ ἀδελφοῦ σου μὴ δῶτε τὸ ἅγιον τοῖς κυσὶ μηδὲ βάλητε τοὺς μαργαρίτας ὑμῶν ἔμπροσθεν τῶν χοίρων μήποτε καταπατήσωσιν αὐτοὺς ἐν τοῖς ποσὶν αὐτῶν καὶ στραφέντες ῥήξωσιν ὑμ\n",
        "\n",
        "# μὴ κρίνετε ἵνα μὴ κριθῆτε ἐν ᾧ γὰρ κρίματι κρίνετε κριθήσεσθε καὶ ἐν ᾧ μέτρῳ μετρεῖτε μετρηθήσεται ὑμῖν τί δὲ βλέπεις τὸ κάρφος τὸ ἐν τῷ ὀφθαλμῷ τοῦ ἀδελφοῦ σου τὴν δὲ ἐν τῷ σῷ ὀφθαλμῷ δοκὸν οὐ\n",
        "# κατανοεῖς ἢ πῶς ἐρεῖς τῷ ἀδελφῷ σου ἄφες ἐκβάλω τὸ κάρφος ἀπὸ τοῦ ὀφθαλμοῦ σου καὶ ἰδοὺ ἡ δοκὸς ἐν τῷ ὀφθαλμῷ σου ὑποκριτά ἔκβαλε πρῶτον τὴν δοκὸν ἐκ τοῦ ὀφθαλμοῦ σου καὶ τότε διαβλέψεις ἐκβαλεῖν τὸ\n",
        "# κάρφος ἐκ τοῦ ὀφθαλμοῦ τοῦ ἀδελφοῦ σου μὴ δῶτε τὸ ἅγιον τοῖς κυσὶ μηδὲ βάλητε τοὺς μαργαρίτας ὑμῶν ἔμπροσθεν τῶν χοίρων μήποτε καταπατήσωσιν αὐτοὺς ἐν τοῖς ποσὶν αὐτῶν καὶ στραφέντες ῥήξωσιν ὑμῶν\n",
        "\n",
        "# μὴ κρίνετε ἵνα μὴ κριθῆτε ἐν ᾧ γὰρ κρίματι κρ--ετε κριθήσεσθε καὶ ἐν ᾧ μέτρῳ με___ῖτε μετρη__σεται ὑμῖν τί -- βλέπεις τὸ κάρφος τὸ ἐν τῷ ὀφθ--μῷ τοῦ ἀδελ__ῦ σου τὴν δὲ ἐν τῷ σῷ ὀφθ__μῷ δοκὸν οὐ\n",
        "# μὴ κρίνετε ἵνα μὴ κριθῆτε ἐν ᾧ γὰρ κρίματι κρίνετε κριθήσεσθε καὶ ἐν ᾧ μέτρῳ μετρεῖτε μετρηθήσεται ὑμῖν τί με βλέπεις τὸ κάρφος τὸ ἐν τῷ ὀφθασμῷ τοῦ ἀδελφοῦ σου τὴν δὲ ἐν τῷ σῷ ὀφθασμῷ δοκὸν οὐ\n",
        "\n",
        "# μὴ κρίνετε ἵνα μὴ κριθῆτε ἐν ᾧ γὰρ κρίματι κρίνετε κριθήσεσθε καὶ ἐν ᾧ μέτρῳ με___ῖτε μετρη__σεται ὑμῖν τί δὲ βλέπεις τὸ κάρφος τὸ ἐν τῷ ὀφθαλμῷ τοῦ ἀδελ__ῦ σου τὴν δὲ ἐν τῷ σῷ ὀφθ__μῷ δοκὸν οὐ\n",
        "# \n",
        "\n",
        "input_text = 'μὴ κρίνετε ἵνα μὴ κριθῆτε ἐν ᾧ γὰρ κρίματι κρίνετε κριθήσεσθε καὶ ἐν ᾧ μέτρῳ με___ῖτε μετρη__σεται ὑμῖν τί δὲ βλέπεις τὸ κάρφος τὸ ἐν τῷ ὀφθαλμῷ τοῦ ἀδελ__ῦ σου τὴν δὲ ἐν τῷ σῷ ὀφθ__μῷ δοκὸν οὐ' #@param {type:\"string\"}\n",
        "\n",
        "# Generate batch from text\n",
        "texts = [Text(sentences=[input_text])]\n",
        "batch = generate_sample(FLAGS, alphabet, texts, pad=True)\n",
        "\n",
        "# Run graph\n",
        "out_tensors = sess.run(graph_tensors, feed_dict={\n",
        "  batch_tf['x']: batch['x'].reshape((1, -1)),\n",
        "  batch_tf['x_len']: batch['x_len'].reshape((1)),\n",
        "  batch_tf['x_word']: batch['x_word'].reshape((1, -1)),\n",
        "  batch_tf['x_word_len']: batch['x_word_len'].reshape((1))\n",
        "})\n",
        "\n",
        "# Convert index outputs to strings\n",
        "x = idx_to_text(batch['x'], alphabet)\n",
        "print('input text: \"{}\"'.format(x))\n",
        "print('predictions:')\n",
        "for beam_i in range(min(20, FLAGS.beam_width)):\n",
        "  y_pred = idx_to_text(out_tensors.sample[0, :, beam_i], alphabet)\n",
        "  print('{}: \"{}\"'.format(beam_i, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zaw_bZwJOgP0"
      },
      "outputs": [],
      "source": [
        "#@title Visualise attention weight\n",
        "\n",
        "input_text = '\\u03bc\\u1f74\\u0020\\u03ba\\u03c1\\u03af\\u03bd\\u03b5\\u03c4\\u03b5\\u0020\\u1f35\\u03bd\\u03b1\\u0020\\u03bc\\u1f74\\u0020\\u03ba\\u03c1\\u03b9\\u03b8\\u1fc6\\u03c4\\u03b5\\u0020\\u1f10\\u03bd\\u0020\\u1fa7\\u0020\\u03b3\\u1f70\\u03c1\\u0020\\u03ba\\u03c1\\u03af\\u03bc\\u03b1\\u03c4\\u03b9\\u0020\\u03ba\\u03c1\\u03af\\u03bd\\u03b5\\u03c4\\u03b5\\u0020\\u03ba\\u03c1\\u03b9\\u03b8\\u03ae\\u03c3\\u03b5\\u03c3\\u03b8\\u03b5\\u0020\\u03ba\\u03b1\\u1f76\\u0020\\u1f10\\u03bd\\u0020\\u1fa7\\u0020\\u03bc\\u03ad\\u03c4\\u03c1\\u1ff3\\u0020\\u03bc\\u03b5\\u005f\\u005f\\u005f\\u1fd6\\u03c4\\u03b5\\u0020\\u03bc\\u03b5\\u03c4\\u03c1\\u03b7\\u005f\\u005f\\u03c3\\u03b5\\u03c4\\u03b1\\u03b9\\u0020\\u1f51\\u03bc\\u1fd6\\u03bd\\u0020\\u03c4\\u03af\\u0020\\u03b4\\u1f72\\u0020\\u03b2\\u03bb\\u03ad\\u03c0\\u03b5\\u03b9\\u03c2\\u0020\\u03c4\\u1f78\\u0020\\u03ba\\u03ac\\u03c1\\u03c6\\u03bf\\u03c2\\u0020\\u03c4\\u1f78\\u0020\\u1f10\\u03bd\\u0020\\u03c4\\u1ff7\\u0020\\u1f40\\u03c6\\u03b8\\u03b1\\u03bb\\u03bc\\u1ff7\\u0020\\u03c4\\u03bf\\u1fe6\\u0020\\u1f00\\u03b4\\u03b5\\u03bb\\u005f\\u005f\\u1fe6\\u0020\\u03c3\\u03bf\\u03c5\\u0020\\u03c4\\u1f74\\u03bd\\u0020\\u03b4\\u1f72\\u0020\\u1f10\\u03bd\\u0020\\u03c4\\u1ff7\\u0020\\u03c3\\u1ff7\\u0020\\u1f40\\u03c6\\u03b8\\u005f\\u005f\\u03bc\\u1ff7\\u0020\\u03b4\\u03bf\\u03ba\\u1f78\\u03bd\\u0020\\u03bf\\u1f50' #@param {type:\"string\"}\n",
        "\n",
        "# Generate batch from text\n",
        "texts = [Text(sentences=[input_text])]\n",
        "batch = generate_sample(FLAGS, alphabet, texts, pad=True)\n",
        "\n",
        "# Run graph\n",
        "out_tensors = sess.run(graph_tensors_greedy, feed_dict={\n",
        "  batch_tf['x']: batch['x'].reshape((1, -1)),\n",
        "  batch_tf['x_len']: batch['x_len'].reshape((1)),\n",
        "  batch_tf['x_word']: batch['x_word'].reshape((1, -1)),\n",
        "  batch_tf['x_word_len']: batch['x_word_len'].reshape((1))\n",
        "})\n",
        "\n",
        "# Convert index outputs to strings\n",
        "x = idx_to_text(batch['x'], alphabet)\n",
        "y_pred = idx_to_text(out_tensors.sample[0], alphabet)\n",
        "\n",
        "# Display\n",
        "html_src = generate_alignment(x, '', y_pred, out_tensors)\n",
        "display(HTML(html_src))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQW0BPQawAhr"
      },
      "outputs": [],
      "source": [
        "#@title Predict all missing parts iteratively\n",
        "\n",
        "input_text = 'μὴ κρίνετε ἵνα μὴ κριθῆτε ἐν ᾧ γὰρ κρίματι κρίνετε κριθήσεσθε καὶ ἐν ᾧ μέτρῳ με---ῖτε μετρη--σεται ὑμῖν τί δὲ βλέπεις τὸ κάρφος τὸ ἐν τῷ ὀφθαλμῷ τοῦ ἀδελ--ῦ σου τὴν δὲ ἐν τῷ σῷ ὀφθ--μῷ δοκὸν οὐ' #@param {type:\"string\"}\n",
        "decoding = 'beam search' #@param [\"beam search\", \"greedy\"]\n",
        "\n",
        "greedy = decoding == 'greedy'\n",
        "x = np.array(list(input_text))\n",
        "\n",
        "print(0, ''.join(x))\n",
        "\n",
        "# Find all chunks of missing characters with max length of FLAGS.pred_char_max\n",
        "for i, m in enumerate(re.finditer(r'-{1,%d}' % FLAGS.pred_char_max, input_text)):\n",
        "\n",
        "  # Replace missing characters with prediction character\n",
        "  x[m.span()[0]:m.span()[1]] = '_'\n",
        "\n",
        "  # If x more than 1000 chars\n",
        "  max_len = 500\n",
        "  max_len_05 = int(max_len / 2)\n",
        "  if len(x) > max_len:\n",
        "    pred_len = m.span()[1] - m.span()[0]\n",
        "    pred_len_05 = int(pred_len / 2)\n",
        "    if m.span()[0] - max_len_05 < 0:\n",
        "      x_start = 0\n",
        "      x_end = max_len\n",
        "    elif m.span()[1] + max_len_05 > len(x):\n",
        "      x_start = len(x) - max_len\n",
        "      x_end = len(x)\n",
        "    else:\n",
        "      x_start = max(0, m.span()[0] - max_len_05 + pred_len_05)\n",
        "      x_end = min(len(x), m.span()[1] + max_len_05 - pred_len_05 - 1)\n",
        "    assert x_end - x_start <= max_len\n",
        "  else:\n",
        "    x_start = 0\n",
        "    x_end = len(x)\n",
        "\n",
        "  # Generate batch from text\n",
        "  texts = [Text(sentences=[''.join(x[x_start:x_end])])]\n",
        "  batch = generate_sample(FLAGS, alphabet, texts, pad=True)\n",
        "\n",
        "  # Run graph\n",
        "\n",
        "  out_tensors = sess.run(\n",
        "      graph_tensors_greedy if greedy else graph_tensors,\n",
        "      feed_dict={\n",
        "        batch_tf['x']: batch['x'].reshape((1, -1)),\n",
        "        batch_tf['x_len']: batch['x_len'].reshape((1)),\n",
        "        batch_tf['x_word']: batch['x_word'].reshape((1, -1)),\n",
        "        batch_tf['x_word_len']: batch['x_word_len'].reshape((1))\n",
        "      })\n",
        "\n",
        "  # Convert index outputs to strings (get the first output of the beam)\n",
        "  if greedy:\n",
        "    y_pred = idx_to_text(out_tensors.sample[0, :], alphabet)\n",
        "  else:\n",
        "    for beam_i in range(min(20, FLAGS.beam_width)):\n",
        "      y_pred = idx_to_text(out_tensors.sample[0, :, beam_i], alphabet)\n",
        "\n",
        "    y_pred = idx_to_text(out_tensors.sample[0, :, 0], alphabet)\n",
        "\n",
        "  # Fill the missing part with prediction\n",
        "  x[m.span()[0]:m.span()[1]] = list(y_pred)\n",
        "\n",
        "  print(i + 1, ''.join(x))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
